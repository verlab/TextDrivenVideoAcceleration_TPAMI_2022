{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpOJ_mAypWdu"
      },
      "source": [
        "Data \\& Code Preparation\n",
        "---\n",
        "\n",
        "If you want to download the code and run it by yourself in your environment, or reproduce our experiments, please follow the next steps:\n",
        "\n",
        "- ### 1. Clone the repo and install the dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hha5O6IkF0Tl"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/verlab/TextDrivenVideoAcceleration_TPAMI_2022\n",
        "%cd TextDrivenVideoAcceleration_TPAMI_2022\n",
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3OYHlSzpwjW"
      },
      "source": [
        "  - ### 2. Prepare the data to train VDAN+\n",
        "\n",
        "  Download \\& Organize the VaTeX Dataset (Annotations and Videos) + Download the Pretrained GloVe Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmV72SfDGyr_"
      },
      "outputs": [],
      "source": [
        "## Download VaTeX JSON data\n",
        "! wget -O semantic_encoding/resources/vatex_training_v1.0.json https://eric-xw.github.io/vatex-website/data/vatex_training_v1.0.json\n",
        "! wget -O semantic_encoding/resources/vatex_validation_v1.0.json https://eric-xw.github.io/vatex-website/data/vatex_validation_v1.0.json\n",
        "\n",
        "## Download the Pretrained GloVe Embeddings\n",
        "! wget -O semantic_encoding/resources/glove.6B.zip http://nlp.stanford.edu/data/glove.6B.zip\n",
        "! unzip -j semantic_encoding/resources/glove.6B.zip glove.6B.300d.txt -d semantic_encoding/resources/\n",
        "! rm semantic_encoding/resources/glove.6B.zip\n",
        "\n",
        "## Download VaTeX Videos (We used the kinetics-datasets-downloader tool to download the available videos from YouTube)\n",
        "# NOTE: VaTeX is composed of the VALIDATION split of the Kinetics-600 dataset; therefore, you must modify the script to download the validation videos only. \n",
        "# We adpated the function download_test_set in the kinetics-datasets-downloader/downloader/download.py file to do so.\n",
        "# 1. Clone repository and copy the modified files\n",
        "! git clone https://github.com/dancelogue/kinetics-datasets-downloader/ semantic_encoding/resources/VaTeX_downloader_files/kinetics-datasets-downloader/\n",
        "! cp semantic_encoding/resources/VaTeX_downloader_files/download.py semantic_encoding/resources/VaTeX_downloader_files/kinetics-datasets-downloader/downloader/download.py\n",
        "! cp semantic_encoding/resources/VaTeX_downloader_files/config.py semantic_encoding/resources/VaTeX_downloader_files/kinetics-datasets-downloader/downloader/lib/config.py\n",
        "\n",
        "# 2. Get the kinetics dataset annotations\n",
        "! wget -O semantic_encoding/resources/VaTeX_downloader_files/kinetics600.tar.gz https://storage.googleapis.com/deepmind-media/Datasets/kinetics600.tar.gz\n",
        "! tar -xf semantic_encoding/resources/VaTeX_downloader_files/kinetics600.tar.gz -C semantic_encoding/resources/VaTeX_downloader_files/\n",
        "! rm semantic_encoding/resources/VaTeX_downloader_files/kinetics600.tar.gz\n",
        "\n",
        "# 3. Download the videos (This can take a while (~28k videos to download)... If you want, you can stop it at any time and train with the downloaded videos)\n",
        "! python3 semantic_encoding/resources/VaTeX_downloader_files/kinetics-datasets-downloader/downloader/download.py --val\n",
        "\n",
        "# Troubleshooting: If the download stops for a long time, experiment increasing the queue size in the parallel downloader (semantic_encoding/resources/VaTeX_downloader_files/kinetics-datasets-downloader/downloader/lib/parallel_download.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGSQj1M7pCSl"
      },
      "source": [
        "### Training VDAN+\n",
        "\n",
        "To train VDAN+, you first need to set up the model and train parameters (current parameters are the same as described in the paper) in the **semantic_encoding/config.py** file, then run the training script **semantic_encoding/train.py**.\n",
        "\n",
        "The training script will save the model in the **semantic_encoding/models** folder.\n",
        "\n",
        "  - ### 1. Setup\n",
        "\n",
        "    ```python\n",
        "        model_params = {\n",
        "            'num_input_frames': 32,\n",
        "            'word_embed_size': 300,\n",
        "            'sent_embed_size': 512,  # h_ij\n",
        "            'doc_embed_size': 512,  # h_i\n",
        "            'hidden_feat_size': 512,\n",
        "            'feat_embed_size': 128,  # d = 128. We also tested with 512 and 1024, but no substantial changes\n",
        "            'sent_rnn_layers': 1,  # Not used in our paper, but feel free to change\n",
        "            'word_rnn_layers': 1,  # Not used in our paper, but feel free to change\n",
        "            'word_att_size': 1024,  # c_p\n",
        "            'sent_att_size': 1024,  # c_d\n",
        "\n",
        "            'use_sentence_level_attention': True,  # Not used in our paper, but feel free to change\n",
        "            'use_word_level_attention': True,  # Not used in our paper, but feel free to change\n",
        "            'use_visual_shortcut': True,  # Uses the R(2+1)D output as the first hidden state (h_0) of the document embedder Bi-GRU.\n",
        "            'learn_first_hidden_vector': False  # Learns the first hidden state (h_0) of the document embedder Bi-GRU.\n",
        "        }\n",
        "\n",
        "        ETA_MARGIN = 0.  # Î· from Equation 1 - (Section 3.1.3 Training)\n",
        "\n",
        "        train_params = {\n",
        "            # VaTeX\n",
        "            'captions_train_fname': 'resources/vatex_training_v1.0.json', # Run semantic_encoding/resources/download_resources.sh first to obtain this file\n",
        "            'captions_val_fname': 'resources/vatex_validation_v1.0.json', # Run semantic_encoding/resources/download_resources.sh first to obtain this file\n",
        "            'train_data_path': 'datasets/VaTeX/raw_videos/', # Download all Kinetics-600 (10-seconds) validation videos using the semantic_encoding/resources/download_vatex_videos.sh script\n",
        "            'val_data_path': 'datasets/VaTeX/raw_videos/', # Download all Kinetics-600 (10-seconds) validation videos using the semantic_encoding/resources/download_vatex_videos.sh script\n",
        "\n",
        "            'embeddings_filename': 'resources/glove.6B.300d.txt', # Run semantic_encoding/resources/download_resources.sh first to obtain this file\n",
        "\n",
        "            'max_sents': 20,  # maximum number of sentences per document\n",
        "            'max_words': 20,  # maximum number of words per sentence\n",
        "\n",
        "            # Training parameters\n",
        "            'train_batch_size': 64, # We used a batch size of 64 (requires a 24Gb GPU card)\n",
        "            'val_batch_size': 64, # We used a batch size of 64 (requires a 24Gb GPU card)\n",
        "            'num_epochs': 100, # We ran in 100 epochs\n",
        "            'learning_rate': 1e-5,\n",
        "            'model_checkpoint_filename': None,  # Add an already trained model to continue training (Leave it as None to train from scratch)...\n",
        "\n",
        "            # Video transformation parameters\n",
        "            'resize_size': (128, 171),  # h, w\n",
        "            'random_crop_size': (112, 112),  # h, w\n",
        "            'do_random_horizontal_flip': True,  # Horizontally flip the whole video randomly in block\n",
        "\n",
        "            # Training process\n",
        "            'optimizer': 'Adam',\n",
        "            'eta_margin': ETA_MARGIN,\n",
        "            'criterion': nn.CosineEmbeddingLoss(ETA_MARGIN),\n",
        "\n",
        "            # Machine and user data\n",
        "            'username': getpass.getuser(),\n",
        "            'hostname': socket.gethostname(),\n",
        "\n",
        "            # Logging parameters\n",
        "            'checkpoint_folder': 'models/',\n",
        "            'log_folder': 'logs/',\n",
        "\n",
        "            # Debugging helpers (speeding things up for debugging)\n",
        "            'use_random_word_embeddings': False,  # Choose if you want to use random embeddings\n",
        "            'train_data_proportion': 1.,  # Choose how much data you want to use for training\n",
        "            'val_data_proportion': 1.,  # Choose how much data you want to use for validation\n",
        "        }\n",
        "\n",
        "        models_paths = {\n",
        "            'VDAN': '<PATH/TO/THE/VDAN/MODEL>', # OPTIONAL: Provide the path to the VDAN model (https://github.com/verlab/StraightToThePoint_CVPR_2020/releases/download/v1.0.0/vdan_pretrained_model.pth) from the CVPR paper: https://github.com/verlab/StraightToThePoint_CVPR_2020/\n",
        "            'VDAN+': '<PATH/TO/THE/VDAN+/MODEL>' # You must fill this path after training the VDAN+ to train the SAFFA agent\n",
        "        }\n",
        "\n",
        "        deep_feats_base_folder = '<PATH/TO/THE/VDAN+EXTRACTED_FEATS/FOLDER>' # Provide the location you stored/want to store your VDAN+ extracted feature vectors\n",
        "    ```\n",
        "\n",
        "  - ### 2. Train\n",
        "\n",
        "    First, make sure you have `punkt` installed..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPsrvF5OOQsi"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw_HLwE7qbHB"
      },
      "source": [
        "  Finally, you're ready to go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gfksp1BfW3Fh"
      },
      "outputs": [],
      "source": [
        "%cd semantic_encoding/\n",
        "! python train.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train_VDAN+.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
